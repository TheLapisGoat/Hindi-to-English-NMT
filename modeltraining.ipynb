{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gensim"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-07-06 21:22:32.960930: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-07-06 21:22:33.810200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1688678555429
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "for device in gpu_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678555593
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678555759
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_w2vec_model = gensim.models.Word2Vec.load('w2vecModels/w2vecModel_trained.hi')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-07-06 21:22:35,340 : INFO : loading Word2Vec object from w2vecModels/w2vecModel_trained.hi\n2023-07-06 21:22:36,253 : INFO : loading wv recursively from w2vecModels/w2vecModel_trained.hi.wv.* with mmap=None\n2023-07-06 21:22:36,254 : INFO : loading vectors from w2vecModels/w2vecModel_trained.hi.wv.vectors.npy with mmap=None\n2023-07-06 21:22:52,805 : INFO : loading syn1neg from w2vecModels/w2vecModel_trained.hi.syn1neg.npy with mmap=None\n2023-07-06 21:23:10,487 : INFO : setting ignored attribute cum_table to None\n2023-07-06 21:23:15,786 : INFO : Word2Vec lifecycle event {'fname': 'w2vecModels/w2vecModel_trained.hi', 'datetime': '2023-07-06T21:23:15.785983', 'gensim': '4.3.1', 'python': '3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1035-azure-x86_64-with-glibc2.31', 'event': 'loaded'}\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678596091
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_kv = hindi_w2vec_model.wv"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678596251
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "temp_vec = copy.copy((hindi_kv.vectors[0]))\n",
        "temp_key = copy.copy(hindi_kv.index_to_key[0])\n",
        "hindi_kv.vectors[0] = np.zeros((200,))\n",
        "hindi_kv.index_to_key[0] = '<pad>'\n",
        "hindi_kv.key_to_index.pop(temp_key)\n",
        "hindi_kv.key_to_index['<pad>'] = 0\n",
        "hindi_kv.add_vector(temp_key, temp_vec)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/tfgpu/lib/python3.10/site-packages/gensim/models/keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n  warnings.warn(\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "750325"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678596492
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_kv.add_vector('<unk>', np.random.randn(200,))\n",
        "hindi_kv.add_vector('<sos>', np.random.randn(200,))\n",
        "hindi_kv.add_vector('<eos>', np.random.randn(200,))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "750328"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678597142
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "english_kv = gensim.downloader.load('glove-wiki-gigaword-100')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-07-06 21:23:16,906 : INFO : loading projection weights from /home/azureuser/gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.gz\n2023-07-06 21:23:43,128 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (400000, 100) matrix of type float32 from /home/azureuser/gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.gz', 'binary': False, 'encoding': 'utf8', 'datetime': '2023-07-06T21:23:43.128146', 'gensim': '4.3.1', 'python': '3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1035-azure-x86_64-with-glibc2.31', 'event': 'load_word2vec_format'}\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678623432
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_vec = copy.copy((english_kv.vectors[0]))\n",
        "temp_key = copy.copy(english_kv.index_to_key[0])\n",
        "english_kv.vectors[0] = np.zeros((100,))\n",
        "english_kv.index_to_key[0] = '<pad>'\n",
        "english_kv.key_to_index.pop(temp_key)\n",
        "english_kv.key_to_index['<pad>'] = 0\n",
        "english_kv.add_vector(temp_key, temp_vec)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "400000"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678623629
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_kv.add_vector('<sos>', np.random.randn(100,))\n",
        "english_kv.add_vector('<eos>', np.random.randn(100,))\n",
        "english_kv.add_vector('<unk>', np.random.randn(100,))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "400003"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678623890
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_kv.save('KeyedVectors/hindi_kv')\n",
        "english_kv.save('KeyedVectors/english_kv')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-07-06 21:23:43,418 : INFO : KeyedVectors lifecycle event {'fname_or_handle': 'KeyedVectors/hindi_kv', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-07-06T21:23:43.418754', 'gensim': '4.3.1', 'python': '3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1035-azure-x86_64-with-glibc2.31', 'event': 'saving'}\n2023-07-06 21:23:43,420 : INFO : storing np array 'vectors' to KeyedVectors/hindi_kv.vectors.npy\n2023-07-06 21:23:49,455 : INFO : saved KeyedVectors/hindi_kv\n2023-07-06 21:23:49,456 : INFO : KeyedVectors lifecycle event {'fname_or_handle': 'KeyedVectors/english_kv', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-07-06T21:23:49.456661', 'gensim': '4.3.1', 'python': '3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1035-azure-x86_64-with-glibc2.31', 'event': 'saving'}\n2023-07-06 21:23:49,457 : INFO : storing np array 'vectors' to KeyedVectors/english_kv.vectors.npy\n2023-07-06 21:23:51,330 : INFO : saved KeyedVectors/english_kv\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678631533
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678631718
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 22\n",
        "BATCH_SIZE = 32"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678631889
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(input_file, target_file):\n",
        "    with open(input_file) as f_input, open(target_file) as f_target:\n",
        "        for input_line, target_line in zip(f_input, f_target):\n",
        "            if input_line == \"\" or target_line == \"\":\n",
        "                continue\n",
        "            enc_input_tokens = word_tokenize(input_line)\n",
        "            dec_input_tokens = word_tokenize(target_line)\n",
        "            target_tokens = word_tokenize(target_line)\n",
        "            if len(enc_input_tokens) > 20 or len(dec_input_tokens) > 20:\n",
        "                continue\n",
        "            enc_input_tokens = ['<sos>'] + enc_input_tokens + ['<eos>']\n",
        "            dec_input_tokens = ['<sos>'] + dec_input_tokens + ['<eos>']\n",
        "            target_tokens = target_tokens + ['<eos>']\n",
        "            for idx, token in enumerate(enc_input_tokens):\n",
        "                if token in hindi_kv:\n",
        "                    enc_input_tokens[idx] = hindi_kv.key_to_index[token]\n",
        "                else:\n",
        "                    enc_input_tokens[idx] = hindi_kv.key_to_index['<unk>']\n",
        "            for idx, token in enumerate(dec_input_tokens):\n",
        "                if token in english_kv:\n",
        "                    dec_input_tokens[idx] = english_kv.key_to_index[token]\n",
        "                else:\n",
        "                    dec_input_tokens[idx] = english_kv.key_to_index['<unk>']\n",
        "            for idx, token in enumerate(target_tokens):\n",
        "                if token in english_kv:\n",
        "                    target_tokens[idx] = english_kv.key_to_index[token]\n",
        "                else:\n",
        "                    target_tokens[idx] = english_kv.key_to_index['<unk>']\n",
        "            enc_input_tokens = enc_input_tokens + [hindi_kv.key_to_index['<pad>']] * (MAX_SEQ_LEN - len(enc_input_tokens))\n",
        "            dec_input_tokens = dec_input_tokens + [english_kv.key_to_index['<pad>']] * (MAX_SEQ_LEN - len(dec_input_tokens))\n",
        "            target_tokens = target_tokens + [english_kv.key_to_index['<pad>']] * (MAX_SEQ_LEN - len(target_tokens))\n",
        "\n",
        "            yield (enc_input_tokens, dec_input_tokens), target_tokens "
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678632068
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_file = \"Data/train_processed.hi\"\n",
        "train_target_file = \"Data/train_processed.en\"\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(train_input_file, train_target_file),\n",
        "    output_signature = (\n",
        "        (tf.TensorSpec(shape = (MAX_SEQ_LEN,), dtype = tf.int32), tf.TensorSpec(shape = (MAX_SEQ_LEN,), dtype = tf.int32)),\n",
        "        tf.TensorSpec(shape = (MAX_SEQ_LEN,), dtype = tf.int32)\n",
        "    )\n",
        ")\n",
        "train_dataset = train_dataset.shuffle(100000, seed = 218)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder = True, num_parallel_calls = tf.data.AUTOTUNE)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-07-06 21:23:52.311962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10766 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678632640
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_file = \"Data/test_processed.hi\"\n",
        "test_target_file = \"Data/test_processed.en\"\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(test_input_file, test_target_file),\n",
        "    output_signature = (\n",
        "        (tf.TensorSpec(shape = (MAX_SEQ_LEN,), dtype = tf.int32), tf.TensorSpec(shape = (MAX_SEQ_LEN,), dtype = tf.int32)),\n",
        "        tf.TensorSpec(shape = (MAX_SEQ_LEN,), dtype = tf.int32)\n",
        "    )\n",
        ")\n",
        "test_dataset = test_dataset.shuffle(10000, seed = 218)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder = True, num_parallel_calls = tf.data.AUTOTUNE)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678632805
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Input, Model\n",
        "from keras.layers import LSTM, Bidirectional, Embedding, Concatenate, Dense, Attention, TimeDistributed\n",
        "from keras.optimizers import Adam"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688725452047
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688725453021
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adam = Adam(learning_rate = 0.001)"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688725461831
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LATENT_DIM = 256\n",
        "\n",
        "encoder_inputs = Input(shape=(MAX_SEQ_LEN,), name = \"Encoder_Inputs\")\n",
        "enc_emb_layer = Embedding(input_dim=hindi_kv.vectors.shape[0], output_dim=hindi_kv.vectors.shape[1], weights=[hindi_kv.vectors], trainable = False, mask_zero = True, name = \"Encoder_Embedding_Layer\")\n",
        "enc_emb = enc_emb_layer(encoder_inputs)\n",
        "encoder_lstm = LSTM(LATENT_DIM, return_sequences = True, return_state=True, name = \"Encoder_STM_Layer\")\n",
        "\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "\n",
        "concat_layer = Concatenate(name = \"Concatenate_Layer\")\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape = (MAX_SEQ_LEN,), name = \"Decoder_Inputs\")\n",
        "dec_emb_layer = Embedding(input_dim=english_kv.vectors.shape[0], output_dim=english_kv.vectors.shape[1], weights=[english_kv.vectors], trainable = True, mask_zero = True, name = \"Decoder_Embedding_Layer\")\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name = \"Decoder_LSTM_Layer\")\n",
        "\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = encoder_states)\n",
        "decoder_attention_layer = Attention(name = \"Attention_Layer\")\n",
        "attn_outputs = decoder_attention_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "attn_decoder_outputs = concat_layer([decoder_outputs, attn_outputs])\n",
        "\n",
        "decoder_dense = TimeDistributed(Dense(english_kv.vectors.shape[0], activation='softmax'), name = \"Dense_Layer\")\n",
        "final_outputs = decoder_dense(attn_decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], final_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer = adam, loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688725465039
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n Encoder_Inputs (InputLayer)    [(None, 22)]         0           []                               \n                                                                                                  \n Decoder_Inputs (InputLayer)    [(None, 22)]         0           []                               \n                                                                                                  \n Encoder_Embedding_Layer (Embed  (None, 22, 200)     150065800   ['Encoder_Inputs[0][0]']         \n ding)                                                                                            \n                                                                                                  \n Decoder_Embedding_Layer (Embed  (None, 22, 100)     40000400    ['Decoder_Inputs[0][0]']         \n ding)                                                                                            \n                                                                                                  \n Encoder_STM_Layer (LSTM)       [(None, 22, 256),    467968      ['Encoder_Embedding_Layer[0][0]']\n                                 (None, 256),                                                     \n                                 (None, 256)]                                                     \n                                                                                                  \n Decoder_LSTM_Layer (LSTM)      [(None, 22, 256),    365568      ['Decoder_Embedding_Layer[0][0]',\n                                 (None, 256),                     'Encoder_STM_Layer[0][1]',      \n                                 (None, 256)]                     'Encoder_STM_Layer[0][2]']      \n                                                                                                  \n Attention_Layer (Attention)    (None, 22, 256)      0           ['Decoder_LSTM_Layer[0][0]',     \n                                                                  'Encoder_STM_Layer[0][0]']      \n                                                                                                  \n Concatenate_Layer (Concatenate  (None, 22, 512)     0           ['Decoder_LSTM_Layer[0][0]',     \n )                                                                'Attention_Layer[0][0]']        \n                                                                                                  \n Dense_Layer (TimeDistributed)  (None, 22, 400004)   205202052   ['Concatenate_Layer[0][0]']      \n                                                                                                  \n==================================================================================================\nTotal params: 396,101,788\nTrainable params: 246,035,988\nNon-trainable params: 150,065,800\n__________________________________________________________________________________________________\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678635866
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\"checkpoints/weights.{epoch:02d}-{val_acc:.2f}.hdf5\", monitor = \"val_acc\")\n",
        "callbacks_list = [model_checkpoint]"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688678636248
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset, epochs = 30, callbacks = callbacks_list, validation_data = test_dataset, steps_per_epoch = 1000, validation_steps = 100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/30\n1000/1000 [==============================] - 1528s 1s/step - loss: 3.9858 - acc: 0.4313 - val_loss: 3.8424 - val_acc: 0.4839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/30\n1000/1000 [==============================] - 1476s 1s/step - loss: 2.3975 - acc: 0.6021 - val_loss: 3.0859 - val_acc: 0.5644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/30\n1000/1000 [==============================] - 1472s 1s/step - loss: 2.2282 - acc: 0.6494 - val_loss: 2.7343 - val_acc: 0.6057\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/30\n1000/1000 [==============================] - 1475s 1s/step - loss: 2.4892 - acc: 0.6182 - val_loss: 2.2431 - val_acc: 0.6389\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/30\n1000/1000 [==============================] - 1489s 1s/step - loss: 3.0009 - acc: 0.5168 - val_loss: 2.0391 - val_acc: 0.6509\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6/30\n1000/1000 [==============================] - 1504s 2s/step - loss: 2.8945 - acc: 0.5056 - val_loss: 1.9165 - val_acc: 0.6633\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 7/30\n1000/1000 [==============================] - 1500s 1s/step - loss: 3.1370 - acc: 0.4950 - val_loss: 1.9338 - val_acc: 0.6575\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 8/30\n1000/1000 [==============================] - 1497s 1s/step - loss: 3.3879 - acc: 0.4713 - val_loss: 1.8451 - val_acc: 0.6645\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 9/30\n1000/1000 [==============================] - 1499s 1s/step - loss: 3.4920 - acc: 0.4590 - val_loss: 1.9380 - val_acc: 0.6537\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 10/30\n1000/1000 [==============================] - 1498s 1s/step - loss: 3.5086 - acc: 0.4557 - val_loss: 1.9393 - val_acc: 0.6488\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 11/30\n1000/1000 [==============================] - 1498s 1s/step - loss: 3.4841 - acc: 0.4524 - val_loss: 1.9284 - val_acc: 0.6521\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 12/30\n1000/1000 [==============================] - 1502s 2s/step - loss: 3.4472 - acc: 0.4542 - val_loss: 2.0066 - val_acc: 0.6391\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 13/30\n1000/1000 [==============================] - 1499s 1s/step - loss: 3.3980 - acc: 0.4671 - val_loss: 1.9876 - val_acc: 0.6381\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 14/30\n1000/1000 [==============================] - 1489s 1s/step - loss: 3.2936 - acc: 0.4955 - val_loss: 2.0256 - val_acc: 0.6360\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 15/30\n1000/1000 [==============================] - 1481s 1s/step - loss: 3.1794 - acc: 0.5249 - val_loss: 2.1122 - val_acc: 0.6217\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 16/30\n1000/1000 [==============================] - 1468s 1s/step - loss: 3.1116 - acc: 0.5475 - val_loss: 2.1313 - val_acc: 0.6218\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 17/30\n1000/1000 [==============================] - 1470s 1s/step - loss: 3.0310 - acc: 0.5737 - val_loss: 2.2478 - val_acc: 0.6053\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 18/30\n1000/1000 [==============================] - 1472s 1s/step - loss: 2.9919 - acc: 0.5892 - val_loss: 2.2669 - val_acc: 0.5965\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 19/30\n1000/1000 [==============================] - 1470s 1s/step - loss: 2.9760 - acc: 0.6024 - val_loss: 2.3359 - val_acc: 0.5986\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 20/30\n1000/1000 [==============================] - 1470s 1s/step - loss: 3.1622 - acc: 0.5678 - val_loss: 2.3730 - val_acc: 0.5882\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 21/30\n1000/1000 [==============================] - 1496s 1s/step - loss: 3.4140 - acc: 0.5014 - val_loss: 2.4503 - val_acc: 0.5787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 22/30\n1000/1000 [==============================] - 1498s 1s/step - loss: 3.3712 - acc: 0.4776 - val_loss: 2.5498 - val_acc: 0.5696\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 23/30\n1000/1000 [==============================] - 1500s 2s/step - loss: 3.1899 - acc: 0.4769 - val_loss: 2.6077 - val_acc: 0.5628\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 24/30\n1000/1000 [==============================] - 1518s 2s/step - loss: 3.1557 - acc: 0.4747 - val_loss: 2.6242 - val_acc: 0.5560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 25/30\n1000/1000 [==============================] - 1513s 2s/step - loss: 3.2656 - acc: 0.4676 - val_loss: 2.6800 - val_acc: 0.5526\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 26/30\n1000/1000 [==============================] - 1519s 2s/step - loss: 3.2532 - acc: 0.4698 - val_loss: 2.7457 - val_acc: 0.5515\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 27/30\n1000/1000 [==============================] - 1505s 2s/step - loss: 3.1625 - acc: 0.4895 - val_loss: 2.7675 - val_acc: 0.5502\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 28/30\n1000/1000 [==============================] - 1516s 2s/step - loss: 3.4185 - acc: 0.4547 - val_loss: 2.7976 - val_acc: 0.5439\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 29/30\n1000/1000 [==============================] - 1526s 2s/step - loss: 3.6211 - acc: 0.4243 - val_loss: 2.8097 - val_acc: 0.5432\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 30/30\n1000/1000 [==============================] - 1552s 2s/step - loss: 3.6823 - acc: 0.4106 - val_loss: 2.9248 - val_acc: 0.5325\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-07-06 21:23:55.443165: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n\t [[{{node Placeholder/_0}}]]\n2023-07-06 21:23:59.674343: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla K80\" frequency: 823 num_cores: 13 environment { key: \"architecture\" value: \"3.7\" } environment { key: \"cuda\" value: \"11080\" } environment { key: \"cudnn\" value: \"8600\" } num_registers: 131072 l1_cache_size: 16384 l2_cache_size: 1572864 shared_memory_size_per_multiprocessor: 114688 memory_size: 11289755648 bandwidth: 240480000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n2023-07-06 21:24:09.990432: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 20922 of 100000\n2023-07-06 21:24:19.990240: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 43536 of 100000\n2023-07-06 21:24:29.990160: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 66309 of 100000\n2023-07-06 21:24:39.990129: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 88705 of 100000\n2023-07-06 21:24:45.177868: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n2023-07-06 21:24:45.208066: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\ntype_id: TFT_OPTIONAL\nargs {\n  type_id: TFT_PRODUCT\n  args {\n    type_id: TFT_TENSOR\n    args {\n      type_id: TFT_INT32\n    }\n  }\n}\n is neither a subtype nor a supertype of the combined inputs preceding it:\ntype_id: TFT_OPTIONAL\nargs {\n  type_id: TFT_PRODUCT\n  args {\n    type_id: TFT_TENSOR\n    args {\n      type_id: TFT_FLOAT\n    }\n  }\n}\n\n\twhile inferring type of node 'cond_19/output/_22'\n2023-07-06 21:24:45.766178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n2023-07-06 21:47:42.596712: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n\t [[{{node Placeholder/_0}}]]\n2023-07-06 21:47:44.230970: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla K80\" frequency: 823 num_cores: 13 environment { key: \"architecture\" value: \"3.7\" } environment { key: \"cuda\" value: \"11080\" } environment { key: \"cudnn\" value: \"8600\" } num_registers: 131072 l1_cache_size: 16384 l2_cache_size: 1572864 shared_memory_size_per_multiprocessor: 114688 memory_size: 11289755648 bandwidth: 240480000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\nBad pipe message: %s [b\"\\xdf.-jC\\xb3\\xc7\\xfd\\xa7\\xc8\\x045\\x07\\xb2\\xe0\\xff\\x93!\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\", b'\\x02']\nBad pipe message: %s [b'\\xd9\\xf9=\\xef\\x0c\\xa1\\x19\\x8a\\x7fA \\xd7I\\x90\\xba\\x9e\\xae\\xfd\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f', b\"\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00\", b'\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08']\nBad pipe message: %s [b'\\x06\\x04\\x01\\x05']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'', b'\\x03\\x03']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'', b'\\x02']\nBad pipe message: %s [b'\\x05\\x02\\x06']\nBad pipe message: %s [b'\\xd2AZp8\\xe9\\xe3\\xd2\\xc6Y1\\xe3\\x0c\\x9d\\xadJ\\x11\\x03\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b']\nBad pipe message: %s [b'\\xb7\\x9c\\x1f\\rj,}\\xfc\\x93t`\\xc2C\\xce\\x8b\\x7fb\\xae\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0']\nBad pipe message: %s [b'\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00']\nBad pipe message: %s [b'\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01']\nBad pipe message: %s [b'J+\\xf1\\x90a\\xe0\\xb2s\\x11(\\xe0\\xddQO\\xc5H\\xad=\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00']\nBad pipe message: %s [b'\\x11\\xc0\\x07\\xc0\\x16\\x00']\nBad pipe message: %s [b'\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b']\nBad pipe message: %s [b'\\xda\\xbav\\x95\\xbe\\xbd\\xa5\\x13-\\xe7']\nBad pipe message: %s [b'\\x8a\\xba\\xee[\\t\\x83]\\x0eR{d\\xc8\\xe9Sb\\x07\\x8c\\xd6\\x00\\x00\\xf4\\xc00\\xc0,\\xc0', b'$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19']\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fb0ec811330>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688723533831
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"lstmModelWeights/model.h5\")"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688723550858
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1909/1909 [==============================] - 1282s 669ms/step - loss: 3.2644 - acc: 0.4699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688725239692
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "tfgpu",
      "language": "python",
      "display_name": "Python (tf-cudnn8.6)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "tfgpu"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}