{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1688664440793
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string, contractions\n",
        "\n",
        "exclude = set(string.punctuation)\n",
        "remove_digits = str.maketrans('', '', string.digits)\n",
        "\n",
        "\n",
        "def clean_hindi(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = ''.join(ch for ch in sentence if ch not in exclude)\n",
        "    sent_temp = ''\n",
        "    for c in sentence:\n",
        "        if c == ' ':\n",
        "            sent_temp += c\n",
        "        elif ord(u'\\u0900') <= ord(c) <= ord(u'\\u097F'):\n",
        "            sent_temp += c\n",
        "    sentence = sent_temp\n",
        "    \n",
        "    sentence = re.sub('[a-z]', '', sentence)\n",
        "    sentence = re.sub('[०१२३४५६७८९।]', '', sentence)\n",
        "    sentence = sentence.translate(remove_digits)\n",
        "    sentence = sentence.strip()\n",
        "    sentence = re.sub(\" +\", \" \", sentence)\n",
        "    return sentence\n",
        "\n",
        "def clean_english(sentence):\n",
        "    expanded_words = []\n",
        "    for word in sentence.split():\n",
        "        expanded_words.append(contractions.fix(word))\n",
        "    sentence = \" \".join(expanded_words)\n",
        "    sentence = sentence.lower()\n",
        "    sentence = ''.join(ch for ch in sentence if ch not in exclude)\n",
        "    sentence = sentence.translate(remove_digits)\n",
        "    sentence = sentence.strip()\n",
        "    sentence = re.sub(\" +\", \" \", sentence)\n",
        "    return sentence"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688664648570
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Data/monolingual-n/monolingual.hi\") as f1, open(\"Data/monolingual_processed.hi\", \"a+\") as f2:\n",
        "    for line in f1:\n",
        "        f2.write(clean_hindi(line) + '\\n')"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688422216677
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Training Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688664451751
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_train = 0\n",
        "count_test = 0\n",
        "np.random.seed = 218\n",
        "with open(\"Data/parallel-n/IITB.en-hi.hi\") as f1, open(\"Data/train_processed.hi\", \"a+\") as f2, open(\"Data/parallel-n/IITB.en-hi.en\") as f3, open(\"data/train_processed.en\", \"a+\") as f4, open(\"Data/test_processed.hi\", \"a+\") as f5, open(\"data/test_processed.en\", \"a+\") as f6:\n",
        "    for line_hi, line_en in zip(f1, f3):\n",
        "        line_hi = clean_hindi(line_hi)\n",
        "        line_en = clean_english(line_en)\n",
        "        if len(word_tokenize(line_hi)) > 20 or len(word_tokenize(line_en)) > 20:\n",
        "            continue\n",
        "        if line_hi != \"\" and line_en != \"\":\n",
        "            rand = np.random.random()\n",
        "            if rand < 0.05:\n",
        "                f5.write(line_hi + '\\n')\n",
        "                f6.write(line_en + '\\n')\n",
        "                count_test += 1\n",
        "            else:\n",
        "                f2.write(line_hi + '\\n')\n",
        "                f4.write(line_en + '\\n')\n",
        "                count_train += 1"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688666182881
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(count_train)\n",
        "print(count_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1152887\n61021\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688666183367
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Data/dev_test/dev.hi\") as f1, open(\"Data/train_processed.hi\", \"a+\") as f2, open(\"Data/dev_test/dev.en\") as f3, open(\"data/train_processed.en\", \"a+\") as f4, open(\"Data/test_processed.hi\", \"a+\") as f5, open(\"data/test_processed.en\", \"a+\") as f6:\n",
        "    for line_hi, line_en in zip(f1, f3):\n",
        "        line_hi = clean_hindi(line_hi)\n",
        "        line_en = clean_english(line_en)\n",
        "        if len(word_tokenize(line_hi)) > 20 or len(word_tokenize(line_en)) > 20:\n",
        "            continue\n",
        "        if line_hi != \"\" and line_en != \"\":\n",
        "            rand = np.random.random()\n",
        "            if rand < 0.05:\n",
        "                f5.write(line_hi + '\\n')\n",
        "                f6.write(line_en + '\\n')\n",
        "                count_test += 1\n",
        "            else:\n",
        "                f2.write(line_hi + '\\n')\n",
        "                f4.write(line_en + '\\n')\n",
        "                count_train += 1"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688666213769
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Data/dev_test/test.hi\") as f1, open(\"Data/train_processed.hi\", \"a+\") as f2, open(\"Data/dev_test/test.en\") as f3, open(\"data/train_processed.en\", \"a+\") as f4, open(\"Data/test_processed.hi\", \"a+\") as f5, open(\"data/test_processed.en\", \"a+\") as f6:\n",
        "    for line_hi, line_en in zip(f1, f3):\n",
        "        line_hi = clean_hindi(line_hi)\n",
        "        line_en = clean_english(line_en)\n",
        "        if len(word_tokenize(line_hi)) > 20 or len(word_tokenize(line_en)) > 20:\n",
        "            continue\n",
        "        if line_hi != \"\" and line_en != \"\":\n",
        "            rand = np.random.random()\n",
        "            if rand < 0.05:\n",
        "                f5.write(line_hi + '\\n')\n",
        "                f6.write(line_en + '\\n')\n",
        "                count_test += 1\n",
        "            else:\n",
        "                f2.write(line_hi + '\\n')\n",
        "                f4.write(line_en + '\\n')\n",
        "                count_train += 1"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688666215696
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(count_train)\n",
        "print(count_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1154384\n61100\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688666218002
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "tfgpu",
      "language": "python",
      "display_name": "Python (tf-cudnn8.6)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "tfgpu"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}